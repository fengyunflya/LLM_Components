{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Unigram Tokenize Algo step by step, and combine steps into one class at end, also compared with transformers method\n",
    "\n",
    "Used in AlBERT, T5, mBART, BigBird, XLNet\n",
    "\n",
    "Reference:\n",
    "https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'▁This': 3, '▁is': 2, '▁the': 1, '▁Hugging': 1, '▁Face': 1, '▁Course.': 1, '▁chapter': 1, '▁about': 1, '▁tokenization.': 1, '▁section': 1, '▁shows': 1, '▁several': 1, '▁tokenizer': 1, '▁algorithms.': 1, '▁Hopefully,': 1, '▁you': 1, '▁will': 1, '▁be': 1, '▁able': 1, '▁to': 1, '▁understand': 1, '▁how': 1, '▁they': 1, '▁are': 1, '▁trained': 1, '▁and': 1, '▁generate': 1, '▁tokens.': 1})\n"
     ]
    }
   ],
   "source": [
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i: j]] += freq\n",
    "\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "token_freqs = list(char_freqs.items()) + sorted_subwords[:300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
      "(['This'], 6.288267030694535)\n"
     ]
    }
   ],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [{\"start\": None, \"score\": None} for _ in range(len(word))]\n",
    "    for start_idx in range(len(word)):\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx: end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                if best_segmentations[end_idx]['score'] is None or best_segmentations[end_idx]['score'] > score:\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "    \n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation['score'] is None:\n",
    "        return [\"<unk>\"], None\n",
    "    \n",
    "    score = segmentation['score']\n",
    "    start = segmentation['start']\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start: end])\n",
    "        next_start = best_segmentations[start]['start']\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start: end])\n",
    "    return tokens, score\n",
    "\n",
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413.10377642940875\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += word_loss * freq\n",
    "    return loss\n",
    "\n",
    "print(compute_loss(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874 0.0\n"
     ]
    }
   ],
   "source": [
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores\n",
    "\n",
    "scores = compute_scores(model)\n",
    "print(scores['ll'], scores['his'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁the', '▁Hugging', '▁Face', '▁', 'c', 'ou', 'r', 's', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offset = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offset]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "print(tokenize(\"This is the Hugging Face course.\", model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine them into one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self, base_tokenizer):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_tokenizer)\n",
    "\n",
    "    def cal_info(self, corpus, base_size):\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            new_words = [word for word, offset in words_with_offsets]\n",
    "            for word in new_words:\n",
    "                self.word_freqs[word] += 1\n",
    "\n",
    "        char_freqs = defaultdict(int)\n",
    "        subwords_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            for i in range(len(word)):\n",
    "                char_freqs[word[i]] += freq\n",
    "                for j in range(i + 2, len(word) + 1):\n",
    "                    subwords_freqs[word[i: j]] += freq\n",
    "        sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.token_freqs = list(char_freqs.items()) + sorted_subwords[: base_size - len(char_freqs)]\n",
    "        self.token_freqs = {token: freq for token, freq in self.token_freqs}\n",
    "\n",
    "    def cal_model(self):\n",
    "        total_sum = sum([freq for token, freq in self.token_freqs.items()])\n",
    "        model = {token: -log(freq / total_sum) for token, freq in self.token_freqs.items()}\n",
    "        return model\n",
    "\n",
    "    def encode_word(self, word, model):\n",
    "        best_segmentation = [{\"start\": 0, \"score\": 1}] + [{\"start\": None, \"score\": None} for _ in range(len(word))]\n",
    "        for start_idx in range(len(word)):\n",
    "            best_score_at_start = best_segmentation[start_idx][\"score\"]\n",
    "            for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "                token = word[start_idx: end_idx]\n",
    "                if token in model and best_score_at_start is not None:\n",
    "                    score = model[token] + best_score_at_start\n",
    "                    if best_segmentation[end_idx][\"score\"] is None or best_segmentation[end_idx][\"score\"] > score:\n",
    "                        best_segmentation[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "        segmentation = best_segmentation[-1]\n",
    "        if segmentation[\"score\"] is None:\n",
    "            return [\"<unk>\"], None\n",
    "        \n",
    "        score = segmentation[\"score\"]\n",
    "        start = segmentation[\"start\"]\n",
    "        end = len(word)\n",
    "        tokens = []\n",
    "        while start != 0:\n",
    "            tokens.insert(0, word[start: end])\n",
    "            next_start = best_segmentation[start][\"start\"]\n",
    "            end = start\n",
    "            start = next_start\n",
    "        tokens.insert(0, word[start: end])\n",
    "        return tokens, score\n",
    "    \n",
    "    def compute_loss(self, model):\n",
    "        loss = 0\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            _, word_loss = self.encode_word(word, model)\n",
    "            loss += word_loss * freq\n",
    "        return loss\n",
    "    \n",
    "    def compute_scores(self, model):\n",
    "        scores = {}\n",
    "        model_loss = self.compute_loss(model)\n",
    "        for token, score in model.items():\n",
    "            if len(token) == 1:\n",
    "                continue\n",
    "            model_without_token = copy.deepcopy(model)\n",
    "            _ = model_without_token.pop(token)\n",
    "            scores[token] = self.compute_loss(model_without_token) - model_loss\n",
    "        return scores\n",
    "    \n",
    "    def train(self, corpus, base_size, target_size, percent_remove=0.1):\n",
    "        self.cal_info(corpus, base_size)\n",
    "        model = self.cal_model()\n",
    "        while len(model) > target_size:\n",
    "            scores = self.compute_scores(model)\n",
    "            sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "            for i in range(int(len(model) * percent_remove)):\n",
    "                _ = self.token_freqs.pop(sorted_scores[i][0])\n",
    "            model = self.cal_model()\n",
    "        self.model = model\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "        encoded_words = [self.encode_word(word, self.model)[0] for word in pre_tokenized_text]\n",
    "        return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁the', '▁Hugging', '▁Face', '▁', 'c', 'ou', 'r', 's', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "new_tokenizer = MyTokenizer('xlnet-base-cased')\n",
    "new_tokenizer.train(corpus, 300, 100)\n",
    "print(new_tokenizer.tokenize('This is the Hugging Face course.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Transformer package, there are 2 methods here, high level api vs low level api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "['▁', 'T', 'h', 'i', 's', '▁', 'i', 's', '▁the', '▁H', 'u', 'g', 'g', 'in', 'g', '▁', 'F', 'a', 'c', 'e', '▁', 'c', 'ou', 'rs', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(corpus, vocab_size=100)\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.tokenize('This is the Hugging Face course.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2\n",
    "\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer, Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"'), normalizers.NFKD(),\n",
    "                                             normalizers.StripAccents(), normalizers.Replace(Regex(\" {2,}\"), \" \")])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(vocab_size=100, special_tokens=special_tokens, unk_token=\"<unk>\")\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "['▁', 'T', 'h', 'i', 's', '▁', 'i', 's', '▁the', '▁H', 'u', 'g', 'g', 'in', 'g', '▁', 'F', 'a', 'c', 'e', '▁', 'c', 'ou', 'rs', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "print(tokenizer.encode('This is the Hugging Face course.').tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
